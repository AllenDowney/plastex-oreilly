% LaTeX source for ``Think Stats:
% Probability and Statistics for Programmers''
% Copyright 2011  Allen B. Downey.

% License: Creative Commons Attribution-NonCommercial 3.0 Unported License.
% http://creativecommons.org/licenses/by-nc/3.0/
%

%\documentclass[10pt,b5paper]{book}
\documentclass[12pt]{book}
\usepackage[width=5.5in,height=8.5in,
  hmarginratio=3:2,vmarginratio=1:1]{geometry}

% for some of these packages, you might have to install
% texlive-latex-extra (in Ubuntu)

\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{mathpazo}

%\usepackage{pslatex}

\usepackage{url}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{setspace}
\usepackage{hevea}                           
\usepackage{upquote}


\title{Think Stats}
\author{Allen B. Downey}

\newcommand{\thetitle}{Think Stats: Probability and Statistics for Programmers}
\newcommand{\theversion}{1.5.9}

% these styles get translated in CSS for the HTML version
\newstyle{a:link}{color:black;}
\newstyle{p+p}{margin-top:1em;margin-bottom:1em}
\newstyle{img}{border:0px}

% change the arrows in the HTML version
\setlinkstext
  {\imgsrc[ALT="Previous"]{back.png}}
  {\imgsrc[ALT="Up"]{up.png}}
  {\imgsrc[ALT="Next"]{next.png}} 

\makeindex

\newif\ifplastex
\plastexfalse

\begin{document}

\frontmatter

\ifplastex
    \usepackage{localdef}
    \maketitle

\else

\input{latexonly}

\newtheoremstyle{exercise}% name of the style to be used
  {\topsep}% measure of space to leave above the theorem. E.g.: 3pt
  {\topsep}% measure of space to leave below the theorem. E.g.: 3pt
  {}% name of font to use in the body of the theorem
  {0pt}% measure of space to indent
  {\bfseries}% name of head font
  {}% punctuation between head and body
  { }% space after theorem head; " " = normal interword space
  {}% Manually specify head

\theoremstyle{exercise}
\newtheorem{exercise}{Exercise}[chapter]

\fi

\newcommand{\tbl}[2]{#2\caption{#1}}


\chapter{Preface}
\label{preface}
\index{floatingindexterm1}

\begin{table}
\vspace*{-6pt}
\def\a{\hphantom{0}}
\def\vrl{\smash{\vrule height43.8pt  width.25pt depth5pt}}
\def\vvrl{\smash{\vrule height58.6pt  width.25pt depth5pt}}
\tbl{Defect rates: what value should go into the lower-right
  corner?\label{tbl:avgaverages}}{%
\begin{tabular}{c@{\hskip9pt}c@{\hskip9pt}c@{\hskip9pt}c@{\hskip9pt}c@{\hskip9pt}c@{\hskip9pt}c}
\toprule
\TCH{Item type}   & & \TCH{Units produced} &&\TCH{Defective units} && \TCH{Defect rate} \\\colrule
A         && \a\a2              && 1               && 0.5\a  \\
B         && \a\a1              && 1               && 1.0\a \\
C         &\vrl& 100            &\vrl& 1               && 0.01 \\\colrule
\multicolumn{5}{l}{\textbf{Total defect rate}}   &\vvrl& \textbf{???}  \\
\botrule
\end{tabular}}
\end{table}


\begin{table}
\def\a{\hphantom{0}}
\def\vrl{\smash{\vrule height48.6pt  width.25pt depth5pt}}
\tbl{Simpson's paradox: applications and admissions 
  by gender of applicant.\label{tbl:simpson}}{%
\begin{tabular}{l@{\hskip9pt}c@{\hskip9pt}c@{\hskip9pt}c@{\hskip9pt}c@{\hskip9pt}c@{\hskip9pt}c}\toprule
 && \TCH{Male} & & \TCH{Female} &&\TCH{Overall} \\\colrule
Department A    && 80/100 $=$ 0.8\hphantom{0}  && $\hphantom{00}$9/10 $=$ 0.9\hphantom{0}    && 89/110 $=$ 0.81 \\
Department B    && $\hphantom{00}$5/10 $=$ 0.5\hphantom{0}  && 60/100 $=$ 0.6\hphantom{0}    && 65/110 $=$ 0.59\hspace*{1.2pt}\\\colrule
\textbf{Total}  && {\bfseries 85}$\mathbf{/}${\bfseries 110} $\mathbf{=}$ {\bfseries 0.77} && {\bfseries 69}$\mathbf{/}${\bfseries
110} $\mathbf{=}$ {\bfseries 0.63} &&
\\\botrule
\end{tabular}}
\end{table}


Fermat's Last Theorem says that there are no integers
$a$, $b$, and $c$ such that

\[ a^n + b^n = c^n \]

for any values of $n$ greater than 2.


For example, this table shows the suits and the corresponding integer
codes:

\begin{tabular}{l c l}
Spades & $\mapsto$ & 3 \\
Hearts & $\mapsto$ & 2 \\
Diamonds & $\mapsto$ & 1 \\
Clubs & $\mapsto$ & 0
\end{tabular}


And here's an equation array:

\begin{eqnarray*}
&&  0! = 1 \\
&&  n! = n (n-1)!
\end{eqnarray*}
%

\section{Comparing tuples}
%\index{floatingindexterm2}
%\index{comparison!tuple}
%\index{tuple!comparison}
%\index{sort method}
%\index{method!sort}

The relational operators work with tuples and other sequences;
Python starts by comparing the first element from each
sequence.  If they are equal, it goes on to the next elements,
and so on, until it finds elements that differ.  Subsequent
elements are not considered (even if they are really big).

\begin{verbatim}
>>> (0, 1, 2) < (0, 3, 4)
True
>>> (0, 1, 2000000) < (0, 3, 4)
True
\end{verbatim}
%
The {\tt sort} function works the same way.  It sorts 
primarily by first element, but in the case of a tie, it sorts
by second element, and so on.  

This feature lends itself to a pattern called {\bf DSU} for 

\begin{description}

\item[Decorate] a sequence by building a list of tuples
with one or more sort keys preceding the elements from the sequence,

\item[Sort] the list of tuples, and

\item[Undecorate] by extracting the sorted elements of the sequence.

\end{description}

\label{DSU}
%\index{DSU pattern}
%\index{pattern!DSU}
%\index{decorate-sort-undecorate pattern}
%\index{pattern!decorate-sort-undecorate}

For example, suppose you have a list of words and you want to
sort them from longest to shortest:

\begin{verbatim}
def sort_by_length(words):
    t = []
    for word in words:
       t.append((len(word), word))

    t.sort(reverse=True)

    res = []
    for length, word in t:
        res.append(word)
    return res
\end{verbatim}
%
The first loop builds a list of tuples, where each
tuple is a word preceded by its length.

{\tt sort} compares the first element, length, first, and
only considers the second element to break ties.  The keyword argument
{\tt reverse=True} tells {\tt sort} to go in decreasing order.
%\index{keyword argument}
%\index{argument!keyword}
%\index{traversal}

The second loop traverses the list of tuples and builds a list of
words in descending order of length.

\begin{exercise}

In this example, ties are broken by comparing words, so words
with the same length appear in reverse alphabetical order.  For other
applications you might want to break ties at random.  Modify
this example so that words with the same length appear in
random order.  Hint: see the {\tt random} function in the
{\tt random} module.
Solution: \url{http://thinkpython.com/code/unstable_sort.py}.

%\index{random module}
%\index{module!random}
%\index{random function}
%\index{function!random}

\end{exercise}

\begin{exercise}
%\index{len function}
%\index{function!len}

Python provides a built-in function called {\tt len} that
returns the length of a string, so the value of \verb"len('allen')" is 5.

Write a function named \verb"right_justify" that takes a string
named {\tt s} as a parameter and prints the string with enough
leading spaces so that the last letter of the string is in column 70
of the display.

\begin{verbatim}
>>> right_justify('allen')
                                                                 allen
\end{verbatim}

\end{exercise}


\section*{Why I wrote this book}
\label{this_section}

Here are some references: Chapter~\ref{preface} 
%and Section~\ref{this_section}

{\em Think Stats: Probability and Statistics for Programmers} is a
textbook for a new kind of introductory prob-stat class.  
It emphasizes the use of statistics to explore large datasets.  It
takes a computational approach, which has several advantages:

\begin{itemize}

\item Students write programs as a way of developing and testing their
  understanding.  For example, they write functions to compute a least
  squares fit, residuals, and the coefficient of determination.
  Writing and testing this code requires them to understand the
  concepts and implicitly corrects misunderstandings.

\item Students run experiments to test statistical behavior.  For
  example, they explore the Central Limit Theorem (CLT) by generating
  samples from several distributions.  When they see that the sum of
  values from a Pareto distribution doesn't converge to normal, they
  remember the assumptions the CLT is based on.

\item Some ideas that are hard to grasp mathematically are easy to
  understand by simulation.  For example, we approximate p-values by
  running Monte Carlo simulations, which reinforces the meaning of the
  p-value.

\item Using discrete distributions and computation makes it possible
  to present topics like Bayesian estimation that are not usually
  covered in an introductory class.  For example, one exercise asks
  students to compute the posterior distribution for the ``German tank
  problem,'' which is difficult analytically but surprisingly easy
  computationally.

\item Because students work in a general-purpose programming language
  (Python), they are able to import data from almost any
  source.  They are not limited to data that has been cleaned and
  formatted for a particular statistics tool.

\end{itemize}

The book lends itself to a project-based approach.  In my class, students
work on a semester-long project that requires them to pose a statistical
question, find a dataset that can address it, and apply each of the
techniques they learn to their own data.

To demonstrate the kind of analysis I want students to do,
the book presents a case study that runs through all of the chapters.
It uses data from two sources:

\begin{itemize}

\item The National Survey of Family Growth (NSFG), conducted by the
  U.S. Centers for Disease Control and Prevention (CDC) to gather
  ``information on family life, marriage and divorce, pregnancy,
  infertility, use of contraception, and men's and women's health.''
  (See \url{http://cdc.gov/nchs/nsfg.htm}.)

\item The Behavioral Risk Factor Surveillance System (BRFSS),
  conducted by the National Center for Chronic Disease Prevention and
  Health Promotion to ``track health conditions and risk behaviors in
  the United States.''  (See \url{http://cdc.gov/BRFSS/}.)

\end{itemize}

Other examples use data from the IRS, the U.S. Census, and
the Boston Marathon.


\section*{How I wrote this book}

When people write a new textbook, they usually start by
reading a stack of old textbooks.  As a result, most books
contain the same material in pretty much the same order.  Often there
are phrases, and errors, that propagate from one book to the next;
Stephen Jay Gould pointed out an example in his essay, ``The Case of
the Creeping Fox Terrier\footnote{A breed of dog that is about half
  the size of a Hyracotherium (see
  \url{http://wikipedia.org/wiki/Hyracotherium}).}.''

I did not do that.  In fact, I used almost no printed material while I
was writing this book, for several reasons:

\begin{itemize}

\item My goal was to explore a new approach to this material, so I didn't
want much exposure to existing approaches.

\item Since I am making this book available under a free license, I wanted
to make sure that no part of it was encumbered by copyright restrictions.

\item Many readers of my books don't have access to libraries of
printed material, so I tried to make references to resources that are
freely available on the Internet.

\item Proponents of old media think that the exclusive
use of electronic resources is lazy and unreliable.  They might be right
about the first part, but I think they are wrong about the second, so
I wanted to test my theory.

% http://www.ala.org/ala/mgrps/rts/nmrt/news/footnotes/may2010/in_defense_of_wikipedia_bonnett.cfm

\end{itemize}

The resource I used more than any other is Wikipedia, the bugbear
of librarians everywhere.  In general, the articles I read on
statistical topics were very good (although I made a few small changes
along the way).  I include references to Wikipedia pages throughout
the book and I encourage you to follow those links; in many cases, the
Wikipedia page picks up where my description leaves off.  The
vocabulary and notation in this book are generally consistent with
Wikipedia, unless I had a good reason to deviate.

Other resources I found useful were Wolfram MathWorld and (of course)
Google.  I also used two books, David MacKay's {\em Information
  Theory, Inference, and Learning Algorithms}, which is the book that
got me hooked on Bayesian statistics, and Press et al.'s {\em
  Numerical Recipes in C}.  But both books are available online,
so I don't feel too bad.

Allen B. Downey \\*
Needham MA \\*

Allen B. Downey is a Professor of Computer Science at 
the Franklin W. Olin College of Engineering.



\end{document}

