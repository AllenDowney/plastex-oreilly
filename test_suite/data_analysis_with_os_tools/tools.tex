%\makeatletter
%\def\Booktitle#1{\gdef\@@Book{#1}}%
%\Booktitle{A P P E N D I X \ \ \ifcase \thechapter\or {O N E} \or  {T W O} \or  {T H R E E} \or
%{F O U R} \or  {F I V E} \or  {S I X} \or   {S E V E N} \or   {E I G H T} \or   {N I N E} \or 
%{T E N} \or   {E L E V E N} \or   {T W E L V E}  \or {T H I R T E E N} \or {F O U R T E E N} \or 
%{F I F T E E N} \or   {S I X T E E N} \or   {S E V E N T E E N} \or {E I G H T E E N} \or   {N I N T E E N} \or 
%{A} \or   {B} \or   {C} \fi}
%\makeatother
 
 
\chapter{Programming Environments for Scientific Computation and Data~Analysis}{}{}
\label{app:tools}
\index{data analysis!software|(} 
\index{software|(} 

% \begin{verse}
%   \itshape  
%   "What do you use, weapons-wise? I am a weapons-man.  They tend to
%   settle the argument."
% 
%   "I don't know. It's a toolbox. You put the tools in for the job."
%   \upshape
% 
% \begin{flushright}
%   Sean Bean and Robert de Niro, \textit{Ronin} (1998)
% \end{flushright}
% \end{verse}

% ============================================================

\vspace*{-6pt}
\Fint{Most data analysis involves a good deal of data manipulation and
numerical computation. Of} course, we use computers for these tasks,
hence we also need appropriate software.

This appendix is intended to give a brief survey of several popular
software systems suitable for the kind of data analysis discussed in
the rest of the book. I am mostly interested in open source software,
although I also mention some of the most important commercial players.

The emphasis here is on \emph{programming environments} for scientific
applications (\ie, libraries or packages intended for general data
manipulation and computation) because being able to operate with data
easily and conveniently is a fundamental capability for all data
analysis efforts. On the other hand, I do not include programs
intended exclusively for graphing data: not because visualization is
not important (it is), but because the choice of plotting or
visualization software is less fundamental.

\vspace*{-6pt}
\section{Software Tools}

\index{software!about}
 
In many ways, our choice of a data manipulation environment determines
what problems we can solve; it certainly determines which problems we
consider to be ``easy'' problems.  For data analysis, the hard problem
that we should be grappling with is always the data and what it is
trying to tell us---the mechanics of handling it should be
sufficiently convenient that we don't even think about them.

Properties I look for in a tool or programming environment include:

\begin{itemize}
\item Low overhead or ceremony; it must be easy to get started on a
  new investigation.
\item Facilitates iterative, interactive use.\pagebreak

\item No arbitrary limitations (within reasonable limits).
\item Scriptable---not strictly required but often nice to have.
\item Stable, correct, mature; free of random defects and other
  annoying distractions.
\end{itemize}

Most of these items are probably not controversial.  Given the
investigative nature of most data analysis, the ability to support
iterative, interactive use is a requirement.  Scriptability and the
absence of arbitrary limitations are both huge enablers. I have been
in situations where the ability to generate and compare hundreds of
graphs revealed obvious similarities and differences that had never
been noticed before---not least because everyone else was using tools
(mostly Excel) that allowed graphs to be created only one at a time.
(Excel is notorious for unnecessarily limiting what can be done, and
so is SQL. Putting even minimal programming abilities on top of SQL
greatly expands the range of problems that can be tackled.)

In addition to these rather obvious requirements, I want to emphasize
two properties that may appear less important, but are, in fact,
essential for successful data analysis. First, it is very important
that the tool or environment itself does not impose much overhead or
``ceremony'': we will be hesitant to investigate an ad hoc idea if our
programming environment is awkward to use or time-consuming to start.
Second, the tool must be stable and correct. Random defects that we
could ``work around'' if we used it as a component in a larger
software project are unacceptable when we use the tool by itself.

In short: whatever we use for data manipulation must not get in our
way! (I consider this more important than how ``sophisticated'' the
tool or environment might be: a dumb tool that works is better than a
cutting-edge solution that does not deliver---a point that is
occasionally a little bit forgotten.)

Before leaving this section, let me remind you that it is not only the
size of the toolbox that matters but also our mastery of the various
elements within it. Only tools we know well enough that using them
feels effortless truly leverage our abilities.  Balancing these
opposing trends (breadth of tool selection and depth of mastery) is a
constant challenge. When in doubt, I recommend you opt for
depth---superficiality does not pay.

% In summary, which particular tool or programming environment you use
% does not matter, as long as you are comfortable and proficient with
% it, it gets the work done, and its limitations do not prevent you from
% doing things that you \emph{should} be doing.
% 
% On the other hand, it does not matter much \emph{which} tools we use
% (as long as their technical limitations are not self-defeating). After
% all, the important aspect is not the tool itself but rather the
% abilities it bestows on us. This point is sometimes forgotten a little
% bit, especially with regard to complex or cutting-edge tools.

\subsection{Scientific Software Is Different}

It is important to realize that scientific software (for a
sufficiently wide definition of ``scientific'') faces some unusual
challenges. First of all, scientific software is \emph{hard}. Writing
high-quality scientific programs is difficult and requires rather rare
and specialized skills. (We'll come back to this later.) Second, the
market for scientific software is \emph{small}, which makes it
correspondingly harder for any one program or vendor to gain critical
mass.

Both of these issues affect all players equally, but a third problem
poses a particular challenge for open source offerings: many users of
scientific software are transients. Graduate students graduate, moving\vadjust{\pagebreak}
on from their projects and often leaving the research environment
entirely. As a result, ``abandonware'' is common among open source
scientific software projects. (And not just there---the long-term
viability of commercial offerings is also far from assured.)

Before investing significant time and effort into mastering any one
tool, it is therefore necessary to evaluate it with regard to two
questions:

\begin{itemize}
\item Is the project of sufficiently high \emph{quality}?
\item Does the project have strong enough \emph{momentum} and
  \emph{support}?\vspace*{-9pt}
\end{itemize}

% ============================================================
\section{A Catalog of Scientific Software}

There are currently three main contenders for interactive, numeric
programming available: Matlab (and its open source clone, Octave), R
(and its commercial predecessor, S/S-Plus), and the NumPy/SciPy set of
libraries for Python. Fundamentally, all three are vector and matrix
packages: they treat vectors and matrices as atomic data types and
allow mathematical functions to operate on them directly (addition,
multiplication, application of a function to all elements in a vector
or matrix). Besides this basic functionality, all three offer various
other mathematical operations, such as special functions, support for
function minimization, or numerical integration and nonlinear equation
solving. It is important to keep in mind that all three are packages
for \emph{numerical} computations that operate with floating-point
numbers. None of these three packages handles \emph{symbolic}
computations, such as the expansion of a function into its Taylor
series. For this you need a symbolic math package, such as
Mathematica or Maple (both commercial) or Maxima, Sage, or Axiom (all
three open source). (Matlab has recently acquired the ability to
perform symbolic operations as well.)

% In addition to these three, there are other players, of course, but
% they tend to be smaller or to cater to more specialized niches. Also,
% while all three of the numerical packages offer similar functionality,
% each one of them has its own share of drawbacks, so that none of them
% comes out as clear winner.

\vspace*{-6pt}
\subsection{Matlab}

\index{software!Matlab}
\index{Matlab}
  
Matlab has been around since the mid-1980s; it has a very large user
base, mostly in the engineering professions but also in pure
mathematics and in the machine-learning community. Rather than do all
the heavy lifting itself, Matlab was conceived as a user-friendly
frontend to existing high-performance numerical linear algebra
libraries (LINPACK and EISPACK, which have been replaced by LAPACK).
Matlab was one of the first widely used languages to treat complex
data structures (such as vectors and matrices) as atomic data types,
allowing the programmer to work with them as if they were scalar
variables and without the need for explicit looping.  (In this day and
age, when object-oriented programming and operator overloading are
commonly used and entirely mainstream, it is hard to imagine how
revolutionary this concept seemed when it was first
developed.\footnote{I remember how blown away I personally was when I
  first read about such features in the programming language APL in
  the mid-1980s!}) In 2008, The\vadjust{\pagebreak} MathWorks (the company that develops
Matlab) acquired the rights to the symbolic math package MuPAD and
incorporated it into subsequent Matlab releases.

Matlab was mainly designed to be used interactively, and its
programming model has serious deficiencies for larger programming
projects. (There are problems with abstraction and encapsulation as
well as memory management issues.) It is a commercial product but
quite reasonably priced.

Matlab places particular emphasis on the quality of its numerical
and floating-point algorithms and implementations.

There is an open source clone of Matlab called Octave. Octave  (\url{http://www.gnu.org/}\break \url{software/octave/}) strives
to be fully compatible; however, there are reports of difficulties
when porting programs back and forth.\vspace*{-12pt}


\subsection{R}

\index{software!R statistical analysis package} 
\index{R statistical analysis package}
 
R is the open source clone of the S/S-Plus statistical package
originally developed at Bell Labs. R (\url{http://www.r-project.org}) has a very large user base,
mostly in the academic statistics community and a healthy tradition
of user-contributed packages. The \emph{Comprehensive R Archive
  Network} (CRAN) is a large central repository of user-contributed
modules.

When first conceived, S was revolutionary in providing an integrated
system for data analysis, including capabilities that we today
associate with scripting languages (built-in support for strings, hash
maps, easy file manipulations, and so on), together with extensive
graphics functionality---and all that in an interactive environment!
On the other hand, S was not conceived as a general-purpose
programming language but is strongly geared toward statistical
applications. Its programming model is quite different from current
mainstream languages, which can make it surprisingly difficult for
someone with a~strong programming background to switch to S (or R).
Finally, its primarily academic outlook makes for a sometimes awkward
fit within a commercial enterprise\break environment.

The strongest feature of R is the large number of built-in (or
user-contributed) functions for primarily \emph{statistical}
calculations.  In contrast to Matlab, R is not intended as a general
numerical workbench (although it can, with some limitations, be used
for that purpose).  Moreover---and perhaps contrary to
expectations---it is not intended as a general-purpose data
manipulation language, although it can serve as scripting language for
text and file manipulations and similar tasks.

A serious problem when working with R is its dated programming model.
It relies strongly on implicit behavior and ``reasonable defaults,''
which leads to particularly opaque programs. Neither the language nor
the libraries provide strong support for organizing information into
larger structures, making it uncommonly difficult to locate pertinent
information.  Although it is easy to pick up isolated ``tricks,'' it
is notoriously difficult to develop a comprehensive understanding of
the whole environment.

Like Matlab, R is here to stay. It has proven its worth (for 30
years!); it is mature; and it has a strong, high-caliber, and vocal
user base. Unlike Matlab, it is free and open source, making it easy
to get started.


\subsection{Python}

\index{software!Python}
\index{Python!about}  

Python has become the scripting language of choice for scientists and
scientific applications, especially in the machine-learning field and in
the biological and social sciences. (Hard-core, large-scale numerical
applications in physics and related fields continue to be done in
C/C++ or even---\textit{horresco referens}---in Fortran.)

The barrier to programming in Python is low, which makes it easy to
start new projects. This is somewhat of a mixed blessing: on the one
hand, there is an abundance of exciting Python projects out there; on
the other hand, they seem to be particularly prone to the
``abandonware'' problem mentioned before. Also, scientists are not
programmers, and it often shows (especially with regard to long-term,
architectural vision and the cultivation of a strong and committed
community).

In addition to a large number of smaller and more specialized
projects, there have been five major attempts to provide a
\emph{comprehensive} Python library for scientific applications.  It
can be confusing to understand how they relate to each other, so they
are summarized here:\footnote{For more information on the history and
  interrelations of these libraries, check out the first chapter in
  Travis B.\ Oliphant's ``Guide to NumPy,'' which can be found on the
  Web.}

\begin{unnumlist}
\subparagraph{Numeric} 
\item This is the original Python module for the manipulation
  of numeric arrays, initiated in 1995 at MIT. Superceded by NumPy. \index{Numeric} 
\subparagraph{Numarray}
\item  An alternative implementation from the Space Telescope
  Science Institute (2001). Considered obsolete, replaced by NumPy. \index{numarray} 
\subparagraph{NumPy}
\item The NumPy project was begun in 2005 to provide a unified \index{software!NumPy} \index{NumPy} 
  framework for numerical matrix calculations. NumPy builds on (and
  supercedes) Numeric, and it includes the additional functionality 
  developed by numarray.  
\subparagraph{SciPy}
\item Started in 2001, the SciPy project evolved out of an\index{software!SciPy}\index{SciPy}  
  effort to combine several previously separate libraries for 
  scientific computing. Builds on and includes NumPy.
\subparagraph{ScientificPython}
\item An earlier (started in 1997) general-purpose\index{software!ScientificPython}\index{ScientificPython} 
  library for scientific applications. In contrast to SciPy, this
  library tries to stay with ``pure Python'' implementations for
  better portability.
\end{unnumlist}

Today, the NumPy/SciPy project has established itself as the clear
winner among general-purpose libraries for scientific applications
in Python, and we will take a closer look at it shortly.

A strong point in favor of Python is the convenient support it has for
relatively fancy and animated graphics. The matplotlib library is the
most commonly used Python library for generating standard plots, and
it has a particularly close relationship with NumPy/SciPy.  Besides
matplotlib there are Chaco and Mayavi (for two- and three-dimensional
graphics, respectively) and libraries such as PyGame and Pyglet (for
animated and interactive graphics)---and, of course, many more.

Uncertainties associated with the future and adoption of Python3
affect all Python projects, but they are particularly critical for
many of the scientific and graphics libraries just mentioned: to
achieve higher performance, these libraries usually rely heavily on C
bindings, which do not port easily to Python3. Coupled with the issue
of ``abandonware'' discussed earlier, this poses a particular
challenge for all scientific libraries based on Python at this time.


\vspace*{-9pt}
\subsubsection{NumPy/SciPy}

\index{software!NumPy} 
\index{NumPy}
\index{software!SciPy}
\index{SciPy}   

The NumPy/SciPy project (\url{http://www.scipy.org}) has become the dominant player in scientific
programming for Python. NumPy provides efficient vector and matrix
operations; SciPy consists of a set of higher-level functions built on
top of NumPy. Together with the matplotlib graphing library and the
IPython interactive shell, NumPy/SciPy provides functionality
resembling Matlab. NumPy/SciPy is open source (BSD-style license) and
has a large user community; it is supported and distributed by a
commercial company (Enthought).

NumPy is intended to contain low-level routines for handling vectors
and matrices, and SciPy is meant to contain all higher-level
functionality.  However, some additional functions are included in
NumPy for backward compatibility, and all NumPy functions are aliased
into the SciPy namespace for convenience. As a result, the
distinction between NumPy and SciPy is not very clear in practice.

% On the face of it, NumPy/SciPy should come out ahead among scientific
% computing environments: combining the wide range of functionality
% provided by SciPy with a modern, general-purpose programming
% environment, and coupled with a strong user base and sponsored by a
% commercial company.

% On closer inspection, NumPy/SciPy do not quite meet these
% expectations. The project does not seem to be managed very
% professionally, with a persistent tendency to emphasize quantity over
% quality.  The list of features claimed in the documentation is
% impressive, but the choice of algorithms, as well as their
% implementations, often appear somewhat amateurish.  The design feels
% unnecessarily complicated, with a very large, but poorly organized API
% that is often awkward to use.
% 
% What worries me most is that although the project has been around for
% almost 10 years and has a large and very active user base, it has
% apparently not been able to achieve and maintain a consistent level of
% reliability and maturity throughout: while for most routines the
% ``happy case'' works just fine, you may find that edge and error cases
% are not handled properly. (The lack of comprehensive unit test
% coverage is openly acknowledged.) Despite occasional efforts in this
% regards, the documentation remains patchy. Features remain not or only
% partially implemented (while at the same time the API continues to
% grow).

NumPy/SciPy can be a lot of fun. It contains a wide selection of
features and is very easy to get started with. Creating graphical
output is simple. Since NumPy/SciPy is built on Python, it is trivial
to integrate it into other software projects. Moreover, it does not
require you to learn (yet another) restricted, special-purpose
language: everything is accessible from a modern, widely used
scripting language.

On the other hand, NumPy/SciPy has its own share of problems. The
project has a tendency to emphasize quantity over quality: the number
of features is very large, but the design appears overly complicated
and is often awkward to use. Edge and error cases are not always
handled properly.
%
On the scientific level, NumPy/SciPy feels amateurish. The choice of
algorithms appears to reflect some well-known textbooks more than 
deep, practical knowledge arising from real experience.

% On closer inspection, NumPy/SciPy is a disappointment. Its biggest
% problem is a persistent tendency to emphasize quantity over quality.
% The huge number of features claimed is misleading: many of them are
% non-essential or superfluous. On the other hand, the project does not
% get the basics right. The design feels overly complicated and is often
% awkward to use. Edge and error cases are not always handled properly.
% Essential functionality is missing. The API is littered with false
% starts (none of which are officially deprecated).

% does not live up to its own ambitions

What worries me most is that the project does not seem to be managed
very well: although it has been around for nearly 10 years and has a
large and active user base, it has apparently not been able to
achieve and maintain a consistent level of reliability and maturity
throughout.
% , or at least clearly distinguish between stable ``core''
% functionality and more experimental features.
% (The lack of comprehensive unit test coverage is openly acknowledged.)
Features seem to be added haphazardly, without any long-term vision or
discernible direction. Despite occasional efforts in this regard, the
documentation remains patchy.  
% Features remain not or only partially
% implemented, while at the same time the API continues to grow.

% http://blog.jarrodmillman.com/
%                       2009/01/when-will-numpy-and-scipy-migrate-to.html
%
% ``A major prerequisite for this transition is excellent test coverage.
% Both NumPy and SciPy currently lack comprehensive test coverage, but
% we are making major improvements in this area. Over the last year, we
% have implemented a new testing framework based on nose and have
% doubled the number of tests for both projects. Over the next year, we
% will need to continue this trend and expand our test coverage even
% more.''
% 
% PKJ: Note that the last sentence does not exactly inspire confidence!

NumPy/SciPy is interesting because, among scientific and numeric
projects, it probably has the lowest barrier to entry and is flexible
and versatile.  That makes it a convenient environment for getting
started and for casual use.  However, because of the overall quality
issues, I would not want to rely on it for ``serious'' production work
at this\break point.

\subsection{What About Java?}

\index{software!Java}
\index{Java}  

% Java is largely a no-show for numerical computation, so much so that a
% Java Numerics Working Group ceased operations years ago (around the
% year 2002) for lack of interest. There are problems with performing
% floating point calculations in Java --- Java Virtual Machine
% encapsulates the underlying platform and therefore makes it impossible
% for programs to take advantage of the floating point hardware.

Java is not a strong player when it comes to heavily \emph{numerical}
computations---so much so that a Java Numerics Working Group ceased
operations years ago (around the year 2002) for lack of interest.

Nevertheless, a lot of production-quality machine-learning programming
is done in Java, where its relatively convenient string handling
(compared to C) and its widespread use for \emph{enterprise}
programming come into play.  One will have to see whether these
applications will over time lead to the development of high-quality
numerical libraries as well.

If you want a comfortable programming environment for large (possibly
distributed) systems that's relatively fast, then Java is a reasonable
choice. However, Java programming has become very heavy-weight (with
tools to manage your frameworks, and so on), which does not encourage
ad hoc, exploratory programming. Groovy carries less programming
overhead but is slow. A last issue concerns Java's traditionally weak
capabilities for interactive graphics and user interfaces, especially
on Linux.

Java is very strong in regard to Big Data; in particular, Hadoop---the
most popular open source map/reduce implementation---is written in
Java. Java is also popular for text processing and searching.

A relatively new project is Incanter (\url{http://incanter.org/}), which uses Clojure (a Lisp
dialect running on top of the Java virtual machine) to develop an
``R-like statistical computing and graphics environment.''  Incanter
is an interesting project, but I don't feel that it has stood the test
of time yet, and one will have to see how it will position itself with
respect to R.

%
% Email from Joe Darcy:
%
%The Java specifications preclude certain classes of numerical 
%optimizations allowed in C and Fortran, whether or not this matters very 
%much depends on the application.  JVM implementations generally don't 
%focus as much numerical benchmarks as on other uses cases.
%


\subsection{Other Players}

The preceding list of programs and packages is, of course, far from
complete. Among the other players, I shall briefly mention three.\pagebreak
%\begin{unnumlist}
\paragraph{SAS}
SAS is a classical statistics packages with strongly\index{software!SAS}\index{SAS}  
established uses in credit scoring and medical trials. SAS was
originally developed for OS/360 mainframes, and it shows.  Its command
language has a distinct 1960s feel, and the whole development cycle is
strongly batch oriented (neither interactive nor exploratory). SAS
works best when well-defined procedures need to be repeated often and
on large data sets. A unique feature of SAS is that it works well with
data sets that are too large to fit into memory and therefore need to
be processed on disk.\vspace*{3pt}

SAS, like the mainframes it used to run on, is very expensive and
requires specially trained operators---it is not for the casual user.
(It is not exactly fun, either. The experience has been described as
comparable to ``scraping down the wallpaper with your fingernails.'')

\paragraph{SciLab}
  SciLab is an open source project similar to\index{software!SciLab}\index{SciLab}  
Matlab. It was created by the French research institute INRIA.

\paragraph{GSL}
The GSL (Gnu Scientific Library) is a C library for\index{software!GSL}\index{GSL (Gnu Scientific Library)}\index{Gnu Scientific Library (GSL)}  
classical numerical analysis: special functions, linear algebra,
nonlinear equations, differential equations, the lot. The GSL was
designed and implemented by a relatively small team of developers, who
clearly knew what they were doing---beyond the standard textbook
treatment. (This is evident from some design choices that specifically
address ugly but important real-world needs.)
%\end{unnumlist}
  
The API is wonderfully clear and consistent, the implementations are
of high quality, and even the documentation is complete and finished.
I find the GSL thoroughly enjoyable to use.  (If you learned numerical
analysis from \textit{Numerical Recipes},\footnote{%
  \cit{Numerical Recipes 3rd Edition: The Art of Scientific Computing}{%
    William H.\ Press, Saul A.\ Teukolsky, William T.\ Vetterling, and
    Brian P.\ Flannery}{Cambridge University Press}{2007}} this is the
software that should have shipped with the book---but didn't.)

The only problem with the GSL is that it is written in C.  You need to
be comfortable with C programming, including memory management and
function pointers, if you want to use it. Bindings to scripting
languages exist, but they are not part of the core project and may not
be as complete or mature as the GSL itself.


\subsection{Recommendations}

So, which to pick? No clear winner emerges, and every single program
or environment has significant (not just superficial) drawbacks.
However, here are some qualified recommendations:
\begin{itemize}
\item Matlab is the 800-pound gorilla of scientific software. As a\index{software!Matlab}\index{Matlab} 
  commercially developed product, it also has a certain amount of
  ``polish'' that many\vadjust{\pagebreak} open source alternatives lack.  If you don't
  have a preferred  programming environment yet, \emph{and} if you can
  afford it (or can make your employer pay for it), then Matlab is
  probably the most comprehensive, most mature, and best supported
  all-purpose tool. Octave is a cheap way to get started and ``try
  before you buy.''

\item If you work with statisticians or have otherwise a need for
  formal statistical methods (tests, models), then R is a serious\index{software!R statistical analysis package}\index{R statistical analysis package} 
  contender.  It can also stand in as a scripting language for data
  manipulation if you don't already have a favorite one yet. Since it
  is open source software, its financial cost to you is zero, but be
  prepared for a significant investment of time and effort before you 
  start feeling comfortable and proficient.

\item NumPy/SciPy is particularly easy to get started with and can be\index{software!NumPy}\index{NumPy}\index{software!SciPy}\index{SciPy} 
  a lot of fun for casual use. However, you may want to evaluate
  carefully whether it will meet your needs in the long run if you are
  planning to use it for a larger or more demanding project.

% \item If you are short on money and long on time and enthusiasm, you
%   might want to give NumPy/SciPy a try. Be prepared for the need to
%   work around defects or missing functionality as required.
%   Evaluate each part of the library that you might want to use
%   individually to determine whether it will meet your needs, and
 
\item NumPy/SciPy, together with some of its associated graphics
  packages, is also of interest if you have a need for fancier,
  possibly interactive, graphics.

\item If you have a need for serious numerical analysis \emph{and}
  you know C well, then the GSL is a mature, high-quality library.
\end{itemize}

I am well aware that this list of options does not cover all
possibilities that may occur in practice!

% ============================================================
\section{Writing Your Own}

\index{software!writing}
 
Given the fragmented tool situation, it may be tempting to write
your own. There is nothing wrong with that: it can be very
effective to write a piece of software specifically for \emph{your}
particular problem and application domain. It is much harder to write
general-purpose scientific software.

Just how much harder is generally underappreciated. When P.\ J.\
Plauger worked on his reference implementation of the standard C
library,\footnote{\cit{The Standard C Library}{P.\ J.\
    Plauger}{Prentice Hall}{1992}} he found that he ``spent about as
much time writing and debugging the functions declared in
\texttt{<math.h>} as [he] did all the rest of this library combined''!
Plauger then went on to state his design goals for his implementation
of those functions.

This should startle you: \emph{design goals}? Why
should a reference implementation need any design goals beyond
faithfully and correctly representing the standard?

The reason is that scientific and numerical routines can fail in more
ways than most people expect. For such routines, correctness is not so
much a binary property, as a floating-point value itself. Numerical
routines have more complicated contracts than \texttt{strlen(char *)}.

My prime example for this kind of problem is the sine function. What
could possibly go wrong with it? It is analytic everywhere, strictly
bounded by $[-1,1]$, perfectly smooth, and with no weird behavior
anywhere.  Nonetheless, it is impossible to evaluate the sine
accurately for sufficiently large values of $x$. The reason is that
the sine sweeps out its entire range of values when $x$ changes by as
little as $2 \pi$. Today's floating-point values carry about 16 digits
of precision. Once $x$ has become so large that all of these digits
are required to represent the value of $x$ to the left of the decimal
point, we are no longer able to resolve the location of $x$ within the
interval of length $2 \pi$ with sufficient precision to be
meaningful---hence the ``value'' returned by $\sin(x)$ is basically
random. In practice, the quality of the results starts to degrade long
before we reach this extreme regime. (More accurately the problem lies
not so much in the implementation of the sine but in the inability to
express its input values with the precision required for obtaining a
meaningful result. This makes no difference for the present argument.)

There are two points to take away here.  First, note how
``correctness'' is a relative quality that can degrade smoothly
depending on circumstances (\ie, the inputs).  Second, you should
register the sense of surprise that a function, which in mathematical
theory is perfectly harmless, can turn nasty in the harsh reality of a
computer program!

Similar examples can be found all over and are not limited to function
evaluations.  In particular for iterative algorithms (and almost all
numerical algorithms are iterative), one needs to monitor and confirm
that all intermediate values are uncorrupted---even in cases where the
final result is perfectly reasonable. (This warning applies to many
matrix operations, for instance.)

The punch line here is that although it is often not hard to produce
an implementation that works well for a limited set of input values
and in a narrow application domain, it is much more difficult to write
routines that work equally well for all possible arguments. It takes a
lot of experience to anticipate all possible applications and provide
built-in diagnostics for likely failure modes. If at all possible,
leave this work to specialists!


\vspace*{-6pt}
% ============================================================
\section{Further Reading}

\subsection{Matlab}

\begin{itemize}
\item \cit{Numerical Computing with MATLAB}{Cleve B.\ Moler}{Revised
    reprint, SIAM}{2008}
  The literature on Matlab is vast. I mention this title because its
  author is Cleve Moler, the guy who started it all.\vspace*{-9pt}
\end{itemize}

\subsection{R}

%\cit{A Beginner's Guide to R}{Alain F.\ Zuur, Elena N.\ Ieno,
%    and Erik H.\ W.\ G.\ Meesters}{Springer}{2009}

\begin{itemize}
\item \cit{A Beginner's Guide to R}{Alain F.\ Zuur, Elena N.\ Ieno,
    and Erik H.\ W.\ G.\ Meesters}{Springer}{2009}
  Probably the most elementary introduction into the mechanics of R. 
  A useful book to get started, but it won't carry you very far. 
  Obviously very hastily produced.

%\item \cit{A Beginner's Guide to R}{Alain F.\ Zuur et al}{Springer}{2009}
%  Probably the most elementary introduction into the mechanics of R. 
%  A useful book to get started, but it won't carry you very far. 
%  Obviously very hastily produced.

\item \cit{R in a Nutshell}{Joseph Adler}{O'Reilly}{2009}
  This is the first book on R that is organized by the \emph{task}
  that you want to perform. This makes it an invaluable resource in
  those situations where you know exactly what you want to do but
  can't find the appropriate commands that will tell R how to do it.
  The first two thirds of the book address data manipulation,
  programming, and graphics in general; the remainder is about
  statistical methods.

\item \cit{Using R for Introductory Statistics}{John Verzani}{Chapman
    \& Hall/CRC}{2004}
  This is probably my favorite introductory text on how to perform
  basic statistical analysis using R.
\end{itemize}

\subsection{NumPy/SciPy}

There is no comprehensive introduction to NumPy/SciPy currently
available that takes a user's perspective.  (The ``Guide to NumPy'' by
Travis Oliphant, which can be found on the NumPy website, is too
concerned with implementation issues.) Some useful bits, together with
an introduction to Python and some other libraries, can be found in
either of the following two books.
\begin{itemize}
\item \cit{Python Scripting for Computational Science}{Hans Petter
    Langtangen}{3rd ed., Springer}{2009}

\item \cit{Beginning Python Visualization: Crafting Visual
    Transformation Scripts}{Shai Vaingast}{Apress}{2009}
\end{itemize}
\index{data analysis!software|)} 
\index{software|)} 
\clearpage
\
\thispagestyle{empty}
\
\clearpage